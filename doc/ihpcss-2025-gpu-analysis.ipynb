{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84479c74-a917-4598-8d52-eb8379bcab88",
   "metadata": {},
   "source": [
    "# GPU Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6392b-7f6e-4991-8e65-15d1f4304fc4",
   "metadata": {},
   "source": [
    "This tutorial will explore how to evaluate and optimize the performance of GPU-accelerated codes.\n",
    "The goal is to:\n",
    "- Understand the expected optimal performance of your code.\n",
    "- Identify how far the actual performance deviates from the optimal.\n",
    "- Explore reasons for performance gaps and practical methods to bridge them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eb221-93e3-4bf1-91fa-06ac55f1b412",
   "metadata": {},
   "source": [
    "The material is based on a distilled version of the NHR@FAU workshop GPU Performance Engineering and designed for a two hour event.\n",
    "Feel free to check out the material on [github](https://github.com/SebastianKuckuk/gpu-performance-engineering) in case you are interested, dates for upcoming public iterations for the European region on the [training overview page](https://hpc.fau.de/teaching/tutorials-and-courses/) of NHR@FAU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f53088-fddb-48dc-893e-de2025452265",
   "metadata": {},
   "source": [
    "# Performance Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d301618-5622-4cb9-b29d-e4aba1c90c93",
   "metadata": {},
   "source": [
    "Performance models aim at relating algorithms, their implementation, hardware characteristics and the expected performance (execution time).\n",
    "One of the most straight-forward ones is the *bottleneck model*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab8451-ed54-4690-a16d-0b47969f7bf8",
   "metadata": {},
   "source": [
    "## Bottleneck Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7aa3ff-3556-45f0-89ca-b5c7a5b2adab",
   "metadata": {},
   "source": [
    "The bottleneck model analyzes potential performance limiters individually, such as:\n",
    "* **DRAM Bandwidth**, i.e. the speed at which data can be transferred from/ to global memory.\n",
    "* **Compute Throughput**, i.e. the rate at which computations can be executed.\n",
    "* **Cache Bandwidth**, i.e. the speed of accessing data from different parts of the memory hierarchy, e.g. L2 cache, L1 cache, or shared memory.\n",
    "\n",
    "Each bottleneck is modeled separately to estimate its contribution to the total execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98ae1b-9cc7-4006-81fd-1883e3233434",
   "metadata": {},
   "source": [
    "In other words: application performance is generally limited by the time required to do the meaningful computations, or by getting data to and from where the compute is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a0686-55ae-4e6b-9831-3157df47f135",
   "metadata": {},
   "source": [
    "# Our Test Application - 2D Stencil Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f615c-d918-481d-a0ec-a2cd265f9d31",
   "metadata": {},
   "source": [
    "## CPU Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9739ef-29c5-42ba-b6a9-c9069494e6ef",
   "metadata": {},
   "source": [
    "We begin with a simple yet widely used benchmark: a 2D stencil application.\n",
    "It can be regarded as a proxy application for (matrix-free) matrix-vector multiplications, which are ubiquitous in HPC applications.\n",
    "\n",
    "A serial baseline *CPU-only* implementation can be found in [stencil-2d-base.cpp](../src/stencil-2d/stencil-2d-base.cpp).\n",
    "Reviewing the implementation reveals these key points:\n",
    "* The main workload is encapsulated in the `stencil2d` function.\n",
    "* The application can be parameterized with command line arguments (c.f. `parseCLA_2d` in [stencil-2d-util.h](../src/stencil-2d/stencil-2d-util.h)):\n",
    "  - **Data type**: `float` or `double`\n",
    "  - **nx, ny**: Grid dimensions, defining total workload (`nx * ny`)\n",
    "  - **nWarmUp**: Number of non-timed warm-up iterations\n",
    "  - **nIt**: Number of timed iterations\n",
    "* Basic diagnostic output of performance data is available via the `printStats` function in [util.h](../src/util.h).\n",
    "  - Key **performance metrics** such as the sustained bandwidth are *estimated*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4429a8c-7bae-424d-80ba-f382caef8201",
   "metadata": {},
   "source": [
    "After reviewing the code, we can use the following commands to compile and execute the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58275e77-f8dd-4c8f-9e9d-84c45a6cd873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 ../src/stencil-2d/stencil-2d-base.cpp -o ../build/stencil-2d-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743f636-9209-4b41-b96e-06953df91d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!../build/stencil-2d-base double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0c0b4-67e5-49e0-b15a-6203adb0a2eb",
   "metadata": {},
   "source": [
    "Next, we introduce OpenMP parallelization to enhance *CPU* performance.\n",
    "The updated version is available in [stencil-2d-omp-host.cpp](../src/stencil-2d/stencil-2d-omp-host.cpp), and can be compiled and executed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb15f7c-1399-45a8-a829-0e943af4b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -fopenmp ../src/stencil-2d/stencil-2d-omp-host.cpp -o ../build/stencil-2d-omp-host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f0924-a553-4ecf-907d-9883a65dce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-omp-host double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1f0d5-aa39-4160-8b98-65d7e7a8e861",
   "metadata": {},
   "source": [
    "Depending on parameters set and hardware used, performance gains may vary, not be present at all, or you might even observe a performance degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034df1e-a2aa-486c-87b3-9d0afef268cc",
   "metadata": {},
   "source": [
    "## First Attempt at GPU Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106be87-a522-4787-8a6c-3ca6be99e7ca",
   "metadata": {},
   "source": [
    "To offload computations to the GPU, we extend the code with OpenMP target offloading.\n",
    "In a first attempt, we limit code changes to the `stencil2d` function.\n",
    "The updated version is in [stencil-2d-omp-target-v0.cpp](../src/stencil-2d/stencil-2d-omp-target-v0.cpp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea851d-1b00-4338-b5fd-d10b216fbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -march=native -std=c++17 -mp=gpu -target=gpu ../src/stencil-2d/stencil-2d-omp-target-v0.cpp -o ../build/stencil-2d-omp-target-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bb294-678e-4218-ad52-5b86e1b006c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-omp-target-v0 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf3783-796d-4062-9b56-4642f99d490a",
   "metadata": {},
   "source": [
    "Surprisingly, initial GPU performance is worse than the CPU baseline.\n",
    "While this outcome might not be surprising to you, especially if you have a background in using OpenMP target offloading, we pretend not to know about the pertaining issues for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea588a-90d4-4e3d-9679-cd887bdffc01",
   "metadata": {},
   "source": [
    "More importantly, we can now already *evaluate* performance, which can be useful to compare different variants of the same application.\n",
    "What is missing, however, are the answers to the following questions:\n",
    "* Could performance be better for this particular hard- and software combination?\n",
    "* If so, how can we pinpoint what optimizations need to be applied where to raise our performance levels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601cc21-26c9-4678-8c84-8da1a2248bc2",
   "metadata": {},
   "source": [
    "# Hardware Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002d89b-e092-4694-9eec-7bdba09e5b87",
   "metadata": {},
   "source": [
    "The bottleneck model, like other performance models, applies to both CPUs and GPUs.\n",
    "Since this course focuses on GPU performance, we will examine the theoretical limits of key GPU bottlenecks.\n",
    "Relevant information can be found in official data sheets and white papers \n",
    "    ([V100](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf),\n",
    "    [A100](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf),\n",
    "    [H100](https://resources.nvidia.com/en-us-hopper-architecture/nvidia-h100-tensor-c)),\n",
    "system documentation, third-party resources (such as [TechPowerUp](https://www.techpowerup.com/gpu-specs/)),\n",
    "and obtained by running micro-benchmarks (see [below](#TODO))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff83dd",
   "metadata": {},
   "source": [
    "\n",
    "|                             |  V100 SXM2 32 GB   | A100 SXM 40 GB \\| 80 GB |    H100 SXM 94 GB     |    H100 SXM5 80 GB    |\n",
    "| --------------------------- | :----------------: | :---------------------: | :-------------------: | :-------------------: |\n",
    "| Availability                |     Bridges-2      |      NHR@FAU Alex       |     NHR@FAU Helma     |       Bridges-2       |\n",
    "| CUDA                        |        7.0         |           8.0           |          9.0          |          9.0          |\n",
    "| #Cores                      | 5120<br/>(80 * 64) |   6912<br/>(108 * 64)   | 16896<br/>(132 * 128) | 16896<br/>(132 * 128) |\n",
    "| FP32 Performance \\[TFLOPS\\] |         16         |           19            |          67           |          67           |\n",
    "| FP64 Performance \\[TFLOPS\\] |         8          |          9.75           |          33           |          33           |\n",
    "| FP64:FP32 Ratio             |        1:2         |           1:2           |          1:2          |          1:2          |\n",
    "| Memory \\[GB\\]               |         32         |        40 \\| 80         |          94           |          80           |\n",
    "| Bandwidth \\[GB/s\\]          |        898         |      1555 \\| 2039       |         3360          |         3360          |\n",
    "| L2 Cache \\[MB\\]             |         6          |           40            |          50           |          50           |\n",
    "| TDP \\[W\\]                   |        250         |           400           |          700          |          700          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07696b90-ebdc-4945-8361-1654eee3b266",
   "metadata": {},
   "source": [
    "It is evident that our first attempt fails to hit any of the major performance limits of our GPU.\n",
    "As such, there is likely significant potential to improve our application's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c44b1",
   "metadata": {},
   "source": [
    "We start our optimization by getting a first overview of system-wide behavior next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81239f28-76f4-4cda-8616-230d0cc2a6c6",
   "metadata": {},
   "source": [
    "# Application Level Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618f912-eddd-4eaa-9b3b-b8f9a7b6443f",
   "metadata": {},
   "source": [
    "We follow a *top-down approach* to performance analysis, starting with a *whole application* performance overview and then narrowing down to specific *hot spots*.\n",
    "An initial overview can be obtained using Nsight Systems, using either solely the command line interface, or by complementing the analysis with the provided GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bae05-e9b9-4846-a5f0-a8db504ac9e6",
   "metadata": {},
   "source": [
    "## Nsight Systems CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1854f-d17e-4e0f-8c03-f385f4a02b34",
   "metadata": {},
   "source": [
    "First, we compile and execute out benchmark application to make sure that results are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799e1cd-e45c-4b01-b07d-b0bb61618a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -march=native -std=c++17 -mp=gpu -target=gpu ../src/stencil-2d/stencil-2d-omp-target-v0.cpp -o ../build/stencil-2d-omp-target-v0\n",
    "!../build/stencil-2d-omp-target-v0 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9a136-7d91-4e14-a655-faeefaf5ec8d",
   "metadata": {},
   "source": [
    "Next, we profile our binary with `nsys profile`.\n",
    "The most important command line arguments are:\n",
    "* `--stats=true`: prints a summary of performance statistics on the command line\n",
    "* `-o ...`: sets the target output profile file\n",
    "* `--force-overwrite=true`: replaces the profile file if it already exists (instead of aborting)\n",
    "\n",
    "A full list of available arguments for the profile mode is available as part of the official [documentation](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#cli-profile-command-switch-options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1b703-f125-4c1a-aff3-b11eb5844715",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/stencil-2d-omp-target-v0 --force-overwrite=true ../build/stencil-2d-omp-target-v0 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1959a-b40c-41d8-81a4-9eb0bf3e9a22",
   "metadata": {},
   "source": [
    "The output of the command line is organized in multiple categories.\n",
    "A possible output for an Nvidia A100 (80 GB) is copied in below (truncated for conciseness and readability):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779e5b7-15bd-4e5b-9ecc-0798ad36e00d",
   "metadata": {},
   "source": [
    "### Possible Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f73a2c-03a9-4976-ae82-11c7cba29d30",
   "metadata": {},
   "source": [
    "```\n",
    "[5/8] Executing 'cuda_api_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)          Name        \n",
    " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  --------------------\n",
    "     53.3      23786192424        516  46097272.1  46176391.0  42338870  51323982    3239367.7  cuMemcpyDtoHAsync_v2\n",
    "     46.7      20848528540        516  40404125.1  40333410.0  40036399  50412082     501856.0  cuMemcpyHtoDAsync_v2\n",
    "      0.0         21000576          1  21000576.0  21000576.0  21000576  21000576          0.0  cuMemAllocManaged   \n",
    "      0.0          8406454        258     32583.2     28026.0     16312     85171      13101.7  cuLaunchKernel      \n",
    "      0.0          1589240          4    397310.0    433931.0     10652    710726     348214.2  cuMemAlloc_v2       \n",
    "      0.0          1434229        258      5559.0      5245.5      3477     36904       2436.3  cuStreamSynchronize \n",
    "      0.0           600084          1    600084.0    600084.0    600084    600084          0.0  cuMemAllocHost_v2   \n",
    "      0.0           237136          1    237136.0    237136.0    237136    237136          0.0  cuModuleLoadDataEx  \n",
    "      0.0             3296          4       824.0       701.5       230      1663        638.4  cuCtxSetCurrent     \n",
    "      0.0             1753          1      1753.0      1753.0      1753      1753          0.0  cuInit              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e47b2",
   "metadata": {},
   "source": [
    "```\n",
    "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                     Name                    \n",
    " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -------------------------------------------\n",
    "    100.0       1620167222        258  6279717.9  6272194.0   6249988   7635683      86899.5  nvkernel__Z9stencil2dIdEvPKT_PS0_mm_F1L5_14\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfd226",
   "metadata": {},
   "source": [
    "```\n",
    "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Count   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
    " --------  ---------------  -----  ----------  ----------  --------  --------  -----------  ----------------------------\n",
    "     51.4      22075935038    516  42782819.8  42666977.0  42240513  44838104     341715.9  [CUDA memcpy Device-to-Host]\n",
    "     48.6      20832112509    516  40372311.1  40298753.0  39997449  50354473     499040.6  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1ed47",
   "metadata": {},
   "source": [
    "```\n",
    "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
    "\n",
    " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
    " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
    " 277025.391    516   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Device-to-Host]\n",
    " 277025.391    516   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d949b-40c0-4941-81de-5296e53a4992",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322092e9-23b4-4ca2-8d9c-dcf9a3203eb9",
   "metadata": {},
   "source": [
    "Looking at the statistics we can see multiple effects:\n",
    "* The number of kernel instances matches our expectation ($2 + 256$).\n",
    "* The number of memory transfers seems to be related to the number of kernel instances.\n",
    "* Comparing the time spent in GPU synchronization (`cuStreamSynchronize`) and kernel execution time shows a mismatch.\n",
    "* Comparing aggregated memory transfer and kernel execution times reveals a stark difference.\n",
    "  * \\> Even if the kernel could be accelerated, overall performance will most likely not increase.\n",
    "  * \\> These numbers could be used to approximate a minimum number of iterations at which the memory transfers get amortized (assuming that the transfer times *don't scale with the number of iterations*).\n",
    "* The size per transfer matches our expectation ($8192^2 \\cdot 8 B \\approx 537 MB$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8b244-8b2c-43be-82a5-a6d97378ccaf",
   "metadata": {},
   "source": [
    "## Nsight Systems GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce39c2-e8cf-4a41-96f4-c00cdbfb1299",
   "metadata": {},
   "source": [
    "Next, we take a closer look at the problematic memory transfers by opening the generated `stencil-2d-omp-target-v0.nsys-rep` report file located in the `profiles` folder.\n",
    "\n",
    "You can download it directly from the folder, or **shift + right-click** on this [link](../profiles/stencil-2d-omp-target-v0.nsys-rep) and select \"Download\".\n",
    "Afterwards, open it up in your local installation of Nsight Systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45602ea8-165a-4a77-abf8-036c97707a8c",
   "metadata": {},
   "source": [
    "As you maybe already suspected, the timeline shows a recurring pattern of\n",
    "* two memory transfers (HtoD),\n",
    "* a kernel call, and\n",
    "* two memory transfers (DtoH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd1593-802f-44da-979c-502c7d2c7ff3",
   "metadata": {},
   "source": [
    "Additionally, `cudaStreamSynchronize` is only called at the end of this pattern, which partly explains the deviation from the kernel execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fd350-da15-4f6e-9ed8-324933cd7b4e",
   "metadata": {},
   "source": [
    "At this stage, we have two options:\n",
    "* Update our bottleneck model to explicitly include PCIe transfers\n",
    "* Optimize our application to eliminate unnecessary data transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7c41e",
   "metadata": {},
   "source": [
    "If we choose the first option, we need to consider the peak bandwidth for the PCIe connection, which depends on the GPU generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9eebc",
   "metadata": {},
   "source": [
    "\n",
    "|                                    |  Volta  | Ampere  | Hopper  | \n",
    "|------------------------------------|:-------:|:-------:|:-------:|\n",
    "| PCIe                               | 3.0 x16 | 4.0 x16 | 5.0 x16 |\n",
    "| Bandwidth (per direction) \\[GB/s\\] |  15.8   |  31.5   |  63.0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5212fa",
   "metadata": {},
   "source": [
    "For this tutorial, however, we will proceed with the second option and focus on reducing redundant transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05389b07-b00e-4b54-9e1b-9817909a5d68",
   "metadata": {},
   "source": [
    "## Stencil Code Optimization - Data Transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733486a-4b93-491f-9bd5-3855434d45e8",
   "metadata": {},
   "source": [
    "Having pinpointed our performance bug, we can now optimize data transfers in our application.\n",
    "One straight-forward way is adding unstructured data primitives in our code, basically spanning a region at whose begin and end data is copied *only one time*.\n",
    "The updated version is available at [stencil-2d-omp-target-v1.cpp](../src/stencil-2d/stencil-2d-omp-target-v1.cpp), and can be compiled, executed and profiled using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071bfdc-9a81-4513-8d1f-b14db5c958c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -march=native -std=c++17 -mp=gpu -target=gpu ../src/stencil-2d/stencil-2d-omp-target-v1.cpp -o ../build/stencil-2d-omp-target-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad266c-79bd-4e3a-bf7f-dcb07a00400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-omp-target-v1 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cac78-d802-4208-8640-eb5ddd436fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/stencil-2d-omp-target-v1 --force-overwrite=true ../build/stencil-2d-omp-target-v1 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a6dd7-5571-4346-9618-b242c1bf277e",
   "metadata": {},
   "source": [
    "### Possible Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfc0cd-aeb6-4400-a739-a63a7fa703b5",
   "metadata": {},
   "source": [
    "```\n",
    "[5/8] Executing 'cuda_api_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)          Name        \n",
    " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  --------------------\n",
    "     89.5       1645737125        260   6329758.2   6268100.0      3938   7623351     631560.0  cuStreamSynchronize \n",
    "      4.8         87537361          2  43768680.5  43768680.5  43395807  44141554     527322.8  cuMemcpyDtoHAsync_v2\n",
    "      4.4         81393811          2  40696905.5  40696905.5  40278531  41115280     591670.9  cuMemcpyHtoDAsync_v2\n",
    "      1.2         21206432          1  21206432.0  21206432.0  21206432  21206432          0.0  cuMemAllocManaged   \n",
    "      0.1          1584641          4    396160.3    421646.5     13778    727570     356926.3  cuMemAlloc_v2       \n",
    "      0.1          1361529        258      5277.2      4654.5      4098     43227       3040.1  cuLaunchKernel      \n",
    "      0.0           573652          1    573652.0    573652.0    573652    573652          0.0  cuMemAllocHost_v2   \n",
    "      0.0           236775          1    236775.0    236775.0    236775    236775          0.0  cuModuleLoadDataEx  \n",
    "      0.0             2776          4       694.0       681.5       411      1002        261.0  cuCtxSetCurrent     \n",
    "      0.0             1844          1      1844.0      1844.0      1844      1844          0.0  cuInit              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157c494",
   "metadata": {},
   "source": [
    "```\n",
    "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                     Name                    \n",
    " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -------------------------------------------\n",
    "    100.0       1644696793        258  6374793.8  6264350.0   6244318   7624708     298503.9  nvkernel__Z9stencil2dIdEvPKT_PS0_mm_F1L5_14\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dce7ee",
   "metadata": {},
   "source": [
    "```\n",
    "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Count   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
    " --------  ---------------  -----  ----------  ----------  --------  --------  -----------  ----------------------------\n",
    "     51.7         86989155      2  43494577.5  43494577.5  43004303  43984852     693352.8  [CUDA memcpy Device-to-Host]\n",
    "     48.3         81288901      2  40644450.5  40644450.5  40230464  41058437     585465.3  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588e25b",
   "metadata": {},
   "source": [
    "```\n",
    "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
    "\n",
    " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
    " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
    "   1073.742      2   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Device-to-Host]\n",
    "   1073.742      2   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35e5df-9466-417f-9a97-cd7a4802ef3f",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd358893-c326-4407-9d07-136af43e9a63",
   "metadata": {},
   "source": [
    "Looking at the statistics we can now compare the effects previously discussed:\n",
    "* The number of kernel instances matches our expectation ($2 + 256$).\n",
    "* ~~The number of memory transfers seems to be related to the number of kernel instances.~~\n",
    "* ~~Comparing the time spent in GPU synchronization (`cuStreamSynchronize`) and kernel execution time shows a mismatch.~~\n",
    "* ~~Comparing aggregated memory transfer and kernel execution times reveals an order of magnitude in difference.~~\n",
    "* The size per transfer matches our expectation ($8192 \\cdot 8192 \\cdot 8 \\text{B} \\approx 537 \\text{MB}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb003cf-e846-44aa-b8bf-ec0f625c7dac",
   "metadata": {},
   "source": [
    "Opening up the generated profile output in our GUI reveals what we already expected: single staging parts at the beginning and the end of our application with multiple kernel calls in between.\n",
    "The output file is once again collected in the `../profiles` folder and accessible via [link](../profiles/stencil-2d-omp-target-v1.nsys-rep)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816ff6c-09ec-46a7-948d-a50a1b1390cf",
   "metadata": {},
   "source": [
    "# Kernel Level Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73461e7c-444f-46d7-b3e4-460f0c54c8af",
   "metadata": {},
   "source": [
    "After analyzing the application-level view and addressing the most pressing issues, we can now focus on identifying which kernels contribute most to the overall execution time.\n",
    "\n",
    "In our case, this step is straightforward: the application contains only a single kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca0328-663d-4f11-a7ac-685ced8eb793",
   "metadata": {},
   "source": [
    "We profile the kernel performance of our application using the CLI of Nsight Compute: `ncu`.\n",
    "Key command line arguments are (all command line arguments are listed in the [documentation](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#profile)):\n",
    "* `-o ...`: sets the target output profile file (equivalent to `nsys`)\n",
    "* `--force-overwrite`: replaces the profile file if it already exists (in contrast to `nsys` no `=true`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48eba6-eb93-417b-9092-28c86c03f07b",
   "metadata": {},
   "source": [
    "By default *every kernel* is profiled.\n",
    "Apart from adapting the number of iterations in our applications, we can further limit the scope of profiled kernels with\n",
    "* `--launch-skip n` or `-s n`: skips the first `n` kernels encountered\n",
    "* `--launch-count n` or `-c n`: limits profiling to the first `n` applicable kernels\n",
    "* `--kernel name` or `-k name`: limits profiling to kernels with the name `name`\n",
    "  * can also be used with regex, e.g. `-k regex:\"stencil[1-3]d\"`\n",
    "  * Nsight Compute also supports kernel renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 -o ../profiles/stencil-2d-omp-target-v1 --force-overwrite ../build/stencil-2d-omp-target-v1 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b67de",
   "metadata": {},
   "source": [
    "If no output file is specified, key profiling insights will be displayed directly in the command line output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d08eb-5811-49dc-afdb-d2f118190a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 ../build/stencil-2d-omp-target-v1 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7134-78b8-4c7d-91d2-3abe1114dda1",
   "metadata": {},
   "source": [
    "### Potential Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de943d-ceed-49f2-bd86-61d9b92b3ad8",
   "metadata": {},
   "source": [
    "```\n",
    "  nvkernel__Z9stencil2dIdEvPKT_PS0_mm_F1L5_14 (64, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.0\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ----------- ------------\n",
    "    Metric Name             Metric Unit Metric Value\n",
    "    ----------------------- ----------- ------------\n",
    "    DRAM Frequency                  Ghz         1.59\n",
    "    SM Frequency                    Ghz         1.15\n",
    "    Elapsed Cycles                cycle      8790515\n",
    "    Memory Throughput                 %        35.44\n",
    "    DRAM Throughput                   %         6.99\n",
    "    Duration                         ms         7.61\n",
    "    L1/TEX Cache Throughput           %        61.19\n",
    "    L2 Cache Throughput               %        23.43\n",
    "    SM Active Cycles              cycle   5089551.15\n",
    "    Compute (SM) Throughput           %         1.10\n",
    "    ----------------------- ----------- ------------\n",
    "\n",
    "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c4ae6",
   "metadata": {},
   "source": [
    "```\n",
    "   Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                   128\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                     64\n",
    "    Registers Per Thread             register/thread              40\n",
    "    Shared Memory Configuration Size           Kbyte           32.77\n",
    "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block        byte/block               0\n",
    "    # SMs                                         SM             108\n",
    "    Stack Size                                                  1024\n",
    "    Threads                                   thread            8192\n",
    "    # TPCs                                                        54\n",
    "    Enabled TPC IDs                                              all\n",
    "    Uses Green Context                                             0\n",
    "    Waves Per SM                                                0.05\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    OPT   Est. Speedup: 40.74%                                                                                          \n",
    "          The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85ae05",
   "metadata": {},
   "source": [
    "```\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           32\n",
    "    Block Limit Registers                 block           12\n",
    "    Block Limit Shared Mem                block           32\n",
    "    Block Limit Warps                     block           16\n",
    "    Theoretical Active Warps per SM        warp           48\n",
    "    Theoretical Occupancy                     %           75\n",
    "    Achieved Occupancy                        %         6.25\n",
    "    Achieved Active Warps Per SM           warp         4.00\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    OPT   Est. Local Speedup: 91.67%                                                                                    \n",
    "          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.2%) can be the       \n",
    "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
    "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
    "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
    "          optimizing occupancy.                                                                                         \n",
    "    ----- --------------------------------------------------------------------------------------------------------------\n",
    "    OPT   Est. Local Speedup: 25%                                                                                       \n",
    "          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      \n",
    "          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      \n",
    "          registers.                                                                                                    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f17bc0",
   "metadata": {},
   "source": [
    "```\n",
    "    Section: GPU and Memory Workload Distribution\n",
    "    -------------------------- ----------- ------------\n",
    "    Metric Name                Metric Unit Metric Value\n",
    "    -------------------------- ----------- ------------\n",
    "    Average DRAM Active Cycles       cycle    847195.70\n",
    "    Total DRAM Elapsed Cycles        cycle    484965248\n",
    "    Average L1 Active Cycles         cycle   5089551.15\n",
    "    Total L1 Elapsed Cycles          cycle    949080162\n",
    "    Average L2 Active Cycles         cycle   8317935.46\n",
    "    Total L2 Elapsed Cycles          cycle    666683360\n",
    "    Average SM Active Cycles         cycle   5089551.15\n",
    "    Total SM Elapsed Cycles          cycle    949080162\n",
    "    Average SMSP Active Cycles       cycle   5089300.81\n",
    "    Total SMSP Elapsed Cycles        cycle   3796320648\n",
    "    -------------------------- ----------- ------------\n",
    "\n",
    "    OPT   Est. Speedup: 24.38%                                                                                          \n",
    "          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   \n",
    "          instance value is 42.09% above the average, while the minimum instance value is 100.00% below the average.    \n",
    "    ----- --------------------------------------------------------------------------------------------------------------\n",
    "    OPT   Est. Speedup: 24.38%                                                                                          \n",
    "          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum \n",
    "          instance value is 42.09% above the average, while the minimum instance value is 100.00% below the average.    \n",
    "    ----- --------------------------------------------------------------------------------------------------------------\n",
    "    OPT   Est. Speedup: 24.38%                                                                                          \n",
    "          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
    "          Maximum instance value is 42.09% above the average, while the minimum instance value is 100.00% below the     \n",
    "          average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b3359-d021-403e-b9a2-056bc364e814",
   "metadata": {},
   "source": [
    "The first section, *GPU Speed Of Light Throughput*, provides a summary of key metrics and potential bottlenecks.\n",
    "The most relevant for our analysis are:\n",
    "* `Duration` of a single kernel execution (in ms)\n",
    "* `DRAM Throughput` as a percentage of the theoretical maximum\n",
    "* `Compute (SM) Throughput` as a percentage of the theoretical maximum\n",
    "\n",
    "By default, Nsight Compute also offers suggestions for improving performance.\n",
    "For example, we might see output such as:\n",
    "```\n",
    "OPT   This kernel grid is too small to fill the available resources on this device,\n",
    "      resulting in only 0.1 full waves across all SMs. \n",
    "      Look at Launch Statistics for more details.\n",
    "```\n",
    "\n",
    "Depending on your background, this may prompt the question: what is a *wave*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af06351",
   "metadata": {},
   "source": [
    "# GPU Architecture 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80689c96",
   "metadata": {},
   "source": [
    "## Example: NVIDIA H100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446cb90",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-chip.png\" alt=\"H100 Chip\" width=\"384px\"/>\n",
    "\n",
    "A fundamental design principle of modern GPUs is their hierarchical structure, which enables high levels of parallelism and scalability.\n",
    "To better understand GPU performance, we will examine the NVIDIA H100 architecture as a representative example of state-of-the-art GPUs.\n",
    "\n",
    "Further details can be found in the official [NVIDIA H100 White Paper](https://resources.nvidia.com/en-us-tensor-core).\n",
    "The following figures and information are also derived from this resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17d973",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-layout.png\" alt=\"H100 Chip Layout with Annotations\" width=\"576px\" style=\"background-color:white\"/>\n",
    "\n",
    "The diagram on the right depicts the 'full configuration' of a single NVIDIA H100 chip.\n",
    "However, not all functional units shown are accessible in production GPUs.\n",
    "For example, the SXM5 version of the H100 features **132 Streaming Multiprocessors (SMs)** enabled for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc687d71",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-sm-layout.png\" alt=\"H100 SM Layout\" width=\"384px\" style=\"background-color:white\"/>\n",
    "\n",
    "Each SM is subdivided into 4 sub partitions (SMSP), as visualized in the figure on the right, each with\n",
    "* 16 INT32 units, for a total of $132 \\cdot 4 \\cdot 16 = 8448$,\n",
    "* 32 FP32 units, for a total of $132 \\cdot 4 \\cdot 32 = 16896$,\n",
    "* 16 FP64 units, for a total of $132 \\cdot 4 \\cdot 16 = 8448$, and\n",
    "* 1 tensor core, for a total of $132 \\cdot 4 = 528$.\n",
    "\n",
    "Each unit is capable of executing one fused-multiply-add (FMA) operation per cycle, with the exception of the tensor cores which can each perform 512 FP16/FP32-mixed-precision FMAs at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36636e29",
   "metadata": {},
   "source": [
    "## Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc22546",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-memory-abstract.png\" alt=\"memory abstraction\" width=\"512px\"/>\n",
    "\n",
    "The GPU memory system is organized into a hierarchy designed to balance capacity, speed, and access latency.\n",
    "Its levels, at the example of the H100, are (latency and bandwidth values provided by [GPU Benches](https://github.com/RRZE-HPC/gpu-benches))\n",
    "- **Registers**\n",
    "    - Scope: Private to each thread.\n",
    "    - Latency: Typically 1 cycle.\n",
    "    - Capacity: Each SM has 256 KB available for registers.\n",
    "- **L1 Cache** and **Shared Memory**\n",
    "    - Scope: Shared among all threads scheduled on a given SM. Threads of a given block can access the same shared memory.\n",
    "    - Latency: ~32 cycles.\n",
    "    - Capacity: Configurable up to 100 KB per SM for shared memory, with the remaining space used for L1 cache (up to 256 KB per SM).\n",
    "- **L2 Cache**\n",
    "    - Scope: Shared among all threads (GPU-wide)\n",
    "    - Latency: ~280 cycles.\n",
    "    - Capacity: 50 MB (two partitions with 25 MB each)\n",
    "- **Global Memory (DRAM)**\n",
    "    - Scope: Global to all threads.\n",
    "    - Latency: ~690 cycles.\n",
    "    - Capacity: 80 or 96 GB\n",
    "    - Bandwidth: up to 3.35 TB/s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d16b7d",
   "metadata": {},
   "source": [
    "## Waves and Occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0dcad",
   "metadata": {},
   "source": [
    "While this detour into GPU architecture was interesting, we still have an open question: what is a *wave*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788594d",
   "metadata": {},
   "source": [
    "To answer this, let's clarify how threads are organized on GPUs:\n",
    "- **OpenMP** organizes *threads* in *teams*.\n",
    "- **CUDA** organizes *threads* in *blocks*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce16bfd",
   "metadata": {},
   "source": [
    "Although OpenMP teams and CUDA thread blocks are not strictly equivalent, compilers typically map teams to blocks when targeting NVIDIA GPUs.\n",
    "These blocks are then assigned to Streaming Multiprocessors (SMs):\n",
    "* Each block is scheduled to one SM.\n",
    "* Each SM can handle multiple blocks concurrently (depending on available resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917663a",
   "metadata": {},
   "source": [
    "Within a block, threads are further grouped into *warps*: groups of 32 threads.\n",
    "Warps are the basic unit of scheduling on the SM sub-partitions (SMSPs).\n",
    "\n",
    "A *wave* refers to the maximum number of blocks that can be resident on the GPU at the same time.\n",
    "It is calculated as the number of SMs times the maximum number of blocks per SM.\n",
    "\n",
    "Having at least one full wave ensures that all resources are fully utilized.\n",
    "Multiple (full) waves work as well, but partial waves may lead to sub-optimal performance.\n",
    "\n",
    "This closely relates to the concept of *occupancy* which is defined as the ratio of active warps to the maximum number of warps that can be scheduled on an SM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744a0a7",
   "metadata": {},
   "source": [
    "# Micro Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbea3ae",
   "metadata": {},
   "source": [
    "At this point, you might wonder whether achieving full waves is always necessary, or if a lower degree of parallelism can sometimes suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a5b59",
   "metadata": {},
   "source": [
    "There are two main ways to approach this question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819ef0c",
   "metadata": {},
   "source": [
    "1. **Conceptual Analysis:** \\\n",
    "To have a GPU fully utilize its computational resources, it requires at least as many active threads as there are execution units for the current data type - typically on the order of ten thousand.\n",
    "However, most applications require additional data transfers between main memory (or at least L2 cache) and the compute units, which introduces high latency.\n",
    "To hide this latency and keep the GPU busy, resources need to be oversubscribed: when some warps are stalled waiting for data, others can be scheduled to execute.\n",
    "In practice this usually relates to an order of magnitude more threads required (on the order of a hundred thousand)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b05a13",
   "metadata": {},
   "source": [
    "2. **Benchmarking:** \\\n",
    "(Micro) benchmarks can help study performance for varying number of threads.\n",
    "They can additionally be used to examine the effects of different data types, memory access patterns, computational intensities, the ratio of compute to data transfer, and more.\\\n",
    "Let's look at two practical examples from the [Accelerated Programming EXamples (APEX)](https://github.com/SebastianKuckuk/apex) collection: a [fused-multiply-add (FMA)](https://github.com/SebastianKuckuk/apex/tree/main/src/benchmark/fma) benchmark to assess computational throughput, and a [copy-stream](https://github.com/SebastianKuckuk/apex/tree/main/src/benchmark/stream) benchmark to evaluate memory bandwidth and data transfer efficiency.\n",
    "Automated benchmarking of both is available as part of the [APEX-Generator](https://github.com/SebastianKuckuk/apex-generator) project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cd7a4",
   "metadata": {},
   "source": [
    "## FMA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24ca0c",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/A100-SXM4-80GB---fma.png\" alt=\"FMA Benchmark Results for A100 80GB\" width=\"600px\"/>\n",
    "\n",
    "On the right, we can see the results for the FMA benchmark on a single A100 80GB and the following effects:\n",
    "* The maximum performance observed is $9.7$ PFLOP/s.\n",
    "    * This is pretty much the theoretical limit given by the data sheet for double precision.\n",
    "* A distinct saw-tooth pattern is visible.\n",
    "    * The first peak is at $26\\,615$ threads, the first drop is at $28\\,526$ threads. \n",
    "    * This is just around $256 \\cdot 108 = 27\\,648$ threads, i.e. when each SM gets exactly one thread block.\n",
    "    * At the first drop, performance nearly halves because the amount of work increases only slightly, but the execution time doubles due to one SM having doubled workload. \n",
    "* Compared to the CUDA version, the compiler seems to adapt block sizes when using OpenMP target offloading. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e84a5f",
   "metadata": {},
   "source": [
    "## Stream Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c81285",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/A100-SXM4-80GB---stream.png\" alt=\"Stream Benchmark Results for A100 80GB\" width=\"600px\"/>\n",
    "\n",
    "On the right, we can see the results for the stream benchmark on a single A100 80GB and the following effects:\n",
    "* The asymptotic bandwidth is around $1\\,750$ GB/s, which is just around $85\\%$ of the theoretical limit of $2\\,040$ GB/s.\n",
    "* The maximum bandwidth is around $2\\,580$ GB/s for $1\\,290\\,948$ threads.\n",
    "    * Each thread moves 16 bytes in the double precision case.\n",
    "    * This corresponds to a total volume of just over $20$ MB, one L2 cache partition on the A100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b752353-b68c-4c49-8201-ec75c1c51ac2",
   "metadata": {},
   "source": [
    "## Stencil Code Optimization - Collapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53509ee-d966-4ad9-837e-193f1949680c",
   "metadata": {},
   "source": [
    "As we have seen in the last section about micro benchmarking, we ideally have hundreds of thousands of threads.\n",
    "Revising the code in [stencil-2d-omp-target-v1](../src/stencil-2d/stencil-2d-omp-target-v1.cpp) shows that the current degree of parallelism is limited by the number of iterations of *the outer loop*, in our case 8190."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fd72a",
   "metadata": {},
   "source": [
    "To increase the number of threads we need to generate more parallelism.\n",
    "Luckily, simply associating more loops with our OpenMP construct serves exactly that purpose.\n",
    "One simple way of achieving this is using the `collapse` clause.\n",
    "The updated code is available at [stencil-2d-omp-target-v2](../src/stencil-2d/stencil-2d-omp-target-v2.cpp), and can be compiled, executed and profiled with the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878ff7c-ecd2-43a4-bccb-6e854662206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -march=native -std=c++17 -mp=gpu -target=gpu ../src/stencil-2d/stencil-2d-omp-target-v2.cpp -o ../build/stencil-2d-omp-target-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0be62b-b4e1-4f61-9615-d2e2d3a16a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-omp-target-v2 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa9181-9b5b-472a-8c73-38a3797d8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 ../build/stencil-2d-omp-target-v2 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ba9b1-083b-4d78-9305-89f7afe69e9b",
   "metadata": {},
   "source": [
    "### Potential Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a6070-edce-455a-b9e3-fcf8c5142a0b",
   "metadata": {},
   "source": [
    "```\n",
    "  nvkernel__Z9stencil2dIdEvPKT_PS0_mm_F1L5_14 (524033, 1, 1)x(128, 1, 1), Context 1, Stream 13, Device 0, CC 8.0\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ----------- ------------\n",
    "    Metric Name             Metric Unit Metric Value\n",
    "    ----------------------- ----------- ------------\n",
    "    DRAM Frequency                  Ghz         1.59\n",
    "    SM Frequency                    Ghz         1.15\n",
    "    Elapsed Cycles                cycle      1058644\n",
    "    Memory Throughput                 %        56.95\n",
    "    DRAM Throughput                   %        56.95\n",
    "    Duration                         us       916.77\n",
    "    L1/TEX Cache Throughput           %        27.80\n",
    "    L2 Cache Throughput               %        81.41\n",
    "    SM Active Cycles              cycle   1055577.11\n",
    "    Compute (SM) Throughput           %        64.20\n",
    "    ----------------------- ----------- ------------\n",
    "\n",
    "    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n",
    "          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b642a5",
   "metadata": {},
   "source": [
    "That already looks a lot better.\n",
    "The next question we might ask is: how accurate are our estimated bandwidth numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0579e99",
   "metadata": {},
   "source": [
    "# Profiling Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34cf72",
   "metadata": {},
   "source": [
    "Fortunately, Nsight Compute can also be used to get accurately *measure* bandwidth values for single kernel executions.\n",
    "While the more manual approach demonstrated next might not be particularly useful for general performance analysis and engineering, it can be quite useful for automated measurements of isolated performance metrics.\n",
    "It also helps in keeping profiling overheads low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856dc5c-700b-4da8-a7e8-9110603b358a",
   "metadata": {},
   "source": [
    "Nvidia publishes additional material on the [metrics structure](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-structure), as well as a [list of key metrics](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-metric-comparison).\n",
    "Additionally, Nsight Compute can list available metrics (in a very long list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3cbf3-435d-449a-bac7-306fb26fa36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --list-metrics > ../profiles/metrics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7531ae-de65-4d9b-b2a9-9c80145eb59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ../profiles/metrics.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56728872-c087-493c-834a-5547d1b65dcc",
   "metadata": {},
   "source": [
    "Since the list is quite long and not particularly easy to navigate, let's review some relevant metrics:\n",
    "* `sm__warps_active.avg.pct_of_peak_sustained_active` displays the achieved occupancy.\n",
    "* `dram__bytes_read` and `dram__bytes_write` correspond to the bytes read from and written to DRAM.\n",
    "    This can be extended with `.sum` to obtain the total volume and `.sum.per_second` to obtain the bandwidth.\n",
    "* `smsp__sass_thread_inst_executed_op_{dadd, dmul, dfma}_pred_on.sum` represents the total executed additions, multiplications and fused multiply-adds in double precision.\n",
    "    The total number of FLOPs in double precision can be computed with `dadd + dmul + 2 * dfma`.\n",
    "* `lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_global_op_{ld, st}.sum` can be used to query L2 load and store volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0a26b",
   "metadata": {},
   "source": [
    "Metrics to be measured can be provided on the command line using the optional argument `--metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501cea3-2f4f-4c41-a4dd-f761c4e0fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --metrics sm__warps_active.avg.pct_of_peak_sustained_active,dram__bytes_read.sum,dram__bytes_write.sum,dram__bytes_read.sum.per_second,dram__bytes_write.sum.per_second,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum,smsp__sass_thread_inst_executed_op_dadd_pred_on.sum.per_second,smsp__sass_thread_inst_executed_op_dmul_pred_on.sum.per_second,smsp__sass_thread_inst_executed_op_dfma_pred_on.sum.per_second ../build/stencil-2d-omp-target-v2 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c08ccf-d7fd-462c-b5e2-dff2a631cbbd",
   "metadata": {},
   "source": [
    "Some general guidelines:\n",
    "* For counting metrics (e.g. bytes transferred) the structure is usually `metric.sum`.\n",
    "* For corresponding rates this can be extended to `metric.sum.per_second`.\n",
    "* Many metrics can be recomputed in terms of percentage of theoretical peak performance with `metric.sum.pct_of_peak_sustained_elapsed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbad30d-f2cc-4981-8e32-0b01f7bbebcf",
   "metadata": {},
   "source": [
    "## Further Nsight Compute Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5131b-9419-447f-b584-fa822216a97b",
   "metadata": {},
   "source": [
    "Providing a custom set of metrics can be quite useful, but also extremely cumbersome.\n",
    "A more coarse-grained way of telling Nsight Compute what to measure is using **sections** or **sets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6eb6f6-385d-4574-bae0-471703758cb5",
   "metadata": {},
   "source": [
    "### Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cc1ec-8f3f-4f5d-99af-1613a43d95e5",
   "metadata": {},
   "source": [
    "The available sections can be queried with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d585431-fb4e-4209-b9f6-8417d9308077",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --list-sections > ../profiles/sections.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd51790-16ac-482a-b4d2-1c06d4ef3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../profiles/sections.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62075510-33cd-416a-9288-00c704b26ef6",
   "metadata": {},
   "source": [
    "and used with the `--section` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ee2e3-e163-47d5-91dc-4f367d8b9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --section=SpeedOfLight ../build/stencil-2d-omp-target-v2 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015674b5-812b-4c0d-8e07-5113ed38934c",
   "metadata": {},
   "source": [
    "### Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c75e1b-0c42-4e02-be8b-943dc6588db8",
   "metadata": {},
   "source": [
    "In many cases, multiple sections are required concurrently.\n",
    "Sets provide an interface for the most relevant combinations.\n",
    "As before, they can be queried directly from `ncu` and then used with the corresponding command line argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5bc7fa-772b-4724-b134-519ae658d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --list-sets > ../profiles/sets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ba231-26dc-4a5f-9083-ea54c0d6bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../profiles/sets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9174d15-157a-4c21-884d-29b0951d4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --set=roofline ../build/stencil-2d-omp-target-v2 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4cc247-ed1c-44c0-bb2a-59270f256538",
   "metadata": {},
   "source": [
    "# Nsight Compute GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe9e65-b0b9-4e02-9f43-d1f40440799a",
   "metadata": {},
   "source": [
    "While the command line interface for Nsight Compute is quite powerful, in many cases a more structured and explorable way of presenting the obtained data is advantageous.\n",
    "In this case, the Nsight Compute GUI can be a helpful tool.\n",
    "We follow this pattern:\n",
    "* Profiling of our application with suitable sections/ sets remotely.\n",
    "* Downloading the profile data.\n",
    "* Opening it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40e931-b377-438b-85aa-9b5f58135ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -march=native -std=c++17 -mp=gpu -target=gpu ../src/stencil-2d/stencil-2d-omp-target-v2.cpp -o ../build/stencil-2d-omp-target-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677e5dc-421c-47e6-9647-142d5e37a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-omp-target-v2 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27010c3e-0b27-4a7d-8897-974ea04e870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --set=full -o ../profiles/stencil-2d-omp-target-v2 --force-overwrite ../build/stencil-2d-omp-target-v2 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db4c2a-5b99-4ff1-a693-9a56df1940cc",
   "metadata": {},
   "source": [
    "As before, the obtained profile file `stencil-2d-omp-target-v2.ncu-rep` can be downloaded directly from the `profiles` folder, or by **shift + right-clicking** this [link](../profiles/stencil-2d-omp-target-v2.ncu-rep).\n",
    "Afterwards, open it up with your local installation of Nsight Compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4f95f",
   "metadata": {},
   "source": [
    "The volume of information can be quite overwhelming.\n",
    "Start by double-clicking on (one of) the displayed kernels.\n",
    "Take a moment to explore the different sections.\n",
    "You can expand them by clicking the little triangle next to them.\n",
    "If you are unsure about certain terms you can check short documentation snippets in the form of tooltips by hovering over what you want to know more about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6a399",
   "metadata": {},
   "source": [
    "Let's focus on the **Memory Workload Analysis** section first.\n",
    "It reveals that the data volume read from the L2 cache is quite high compared to the one from DRAM (about a factor of 3x).\n",
    "To understand why this is happening, we first have to remember the thread distribution employed by OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7d2e9",
   "metadata": {},
   "source": [
    "The resulting access pattern leads to about three times as much data being read from L2, which fits the measured values.\n",
    "To remedy this behavior, using (spatial) blocking techniques can be one option.\n",
    "Another is switching to CUDA which naturally allows having a 2D thread decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b9ba3",
   "metadata": {},
   "source": [
    "# Stencil Code Optimization - Spatial Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e72a9",
   "metadata": {},
   "source": [
    "[stencil-2d-cuda-v3](../src/stencil-2d/stencil-2d-cuda-v3.cu) implements the approach detailed above.\n",
    "As ususal, the next cells can be used to compile and execute it.\n",
    "Also note the switch from `nvc++` to `nvcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 ../src/stencil-2d/stencil-2d-cuda-v3.cu -o ../build/stencil-2d-cuda-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-cuda-v3 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ba4b7-80d7-4206-8f59-ab83fea76927",
   "metadata": {},
   "source": [
    "Using our CUDA implementation seems to have reduced performance again.\n",
    "For now, we pretend we don't know what is going on, even if you already spotted the (performance) bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d15f79",
   "metadata": {},
   "source": [
    "We start by checking for the same issue as before - maybe there are additional memory transfers again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fa49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/stencil-2d-cuda-v3 --force-overwrite=true ../build/stencil-2d-cuda-v3 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24226b84",
   "metadata": {},
   "source": [
    "### Potential Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d392d6",
   "metadata": {},
   "source": [
    "```\n",
    "[5/8] Executing 'cuda_api_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
    " --------  ---------------  ---------  ------------  ------------  --------  -----------  ------------  ----------------------\n",
    "     95.8      12198873816          2  6099436908.0  6099436908.0  95515042  12103358774  8490827730.3  cudaDeviceSynchronize \n",
    "      3.0        377821142          2   188910571.0   188910571.0  96573958    281247184   130583690.4  cudaMallocHost        \n",
    "      0.6         81727359          4    20431839.8    20350178.5  20163428     20863574      312054.0  cudaMemcpy            \n",
    "      0.6         70621881          2    35310940.5    35310940.5  34722770     35899111      831798.7  cudaFreeHost          \n",
    "      0.0          3533005        258       13693.8        2560.0      2425      2830601      176058.3  cudaLaunchKernel      \n",
    "      0.0          1442164          2      721082.0      721082.0    720767       721397         445.5  cudaFree              \n",
    "      0.0          1143135          2      571567.5      571567.5    322668       820467      351997.0  cudaMalloc            \n",
    "      0.0              772          1         772.0         772.0       772          772           0.0  cuModuleGetLoadingMode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6a49f",
   "metadata": {},
   "source": [
    "```\n",
    "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                   Name                                 \n",
    " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------------------------------\n",
    "    100.0      12199314275        258  47284163.9  47305954.0  46507675  49070476     289519.5  void stencil2d<double>(const T1 *, T1 *, unsigned long, unsigned long)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6492397",
   "metadata": {},
   "source": [
    "```\n",
    "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
    "\n",
    " Time (%)  Total Time (ns)  Count   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
    " --------  ---------------  -----  ----------  ----------  --------  --------  -----------  ----------------------------\n",
    "     50.6         41222752      2  20611376.0  20611376.0  20434207  20788545     250554.8  [CUDA memcpy Device-to-Host]\n",
    "     49.4         40298038      2  20149019.0  20149019.0  20146011  20152027       4254.0  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d343b93",
   "metadata": {},
   "source": [
    "```\n",
    "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
    "\n",
    " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
    " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
    "   1073.742      2   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Device-to-Host]\n",
    "   1073.742      2   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Host-to-Device]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ddf42",
   "metadata": {},
   "source": [
    "Nsight Systems reveals that there are no spurious transfers and that the majority of the time is indeed spent in executing the kernel(s).\n",
    "Next, we create a profile with Nsight Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3245f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --set=full -o ../profiles/stencil-2d-cuda-v3 --force-overwrite ../build/stencil-2d-cuda-v3 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e670f",
   "metadata": {},
   "source": [
    "After downloading the [report file](../profiles/stencil-2d-cuda-v3.ncu-rep) and opening it locally, we compare this profile with the last OpenMP one.\n",
    "To do so, we can click on `Compare` at the top right and then on `Add Baseline`.\n",
    "After opening the profile for the CUDA version, we once again focus on the memory workload section.\n",
    "We can see multiple effects:\n",
    "* The data volume read from L2 is now a lot lower (about 2x).\n",
    "* The data volume read from and written to DRAM is now about zero.\n",
    "* There is a new data path to system memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3402b",
   "metadata": {},
   "source": [
    "# Final Stencil Code Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d62e82-1ae9-4cf0-b946-d777e2c025fc",
   "metadata": {},
   "source": [
    "The previous analysis reports data transfers from and to system memory, or in other words the host memory, instead of using the GPU memory.\n",
    "Reviewing the code reveals the culprit: instead of device pointers, we pass host pointers to the kernel.\n",
    "That is an easy fix - an updated code version is available at [stencil-2d-cuda-v4](../src/stencil-2d/stencil-2d-cuda-v4.cu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 ../src/stencil-2d/stencil-2d-cuda-v4.cu -o ../build/stencil-2d-cuda-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59884aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stencil-2d-cuda-v4 double 8192 8192 2 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54436c79-a560-4d81-91bf-ef341859b3c5",
   "metadata": {},
   "source": [
    "The performance of our benchmark application is now quite close to the theoretical limit.\n",
    "Further optimization of the implementation will likely not bring any performance benefits.\n",
    "To further accelerate the application, *algorithmic optimizations* are required, e.g. temporal blocking or switching to a different numerical solver algorithm.\n",
    "\n",
    "Nsight compute also shows traces of this when checking the roofline section (download the profile [here](../profiles/stencil-2d-cuda-v4.ncu-rep)) after running the next cell.\n",
    "You can review it in the **GPU Speed of Light Throughput** section by choosing **Roofline Double Precision** in the top right drop-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030e7df-ed0d-4122-aded-e4851f24bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --set=full -o ../profiles/stencil-2d-cuda-v4 --force-overwrite ../build/stencil-2d-cuda-v4 double 8192 8192 2 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b889d",
   "metadata": {},
   "source": [
    "Roofline models are an extended version of the discussed bottleneck model.\n",
    "Further details are beyond the scope of this introduction but, generally speaking: points in the chart that are close to the roofline indicate that the profiled code utilizes the hardware well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e5664-a0ff-4a54-a7d0-8937665cc9b7",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10115f2",
   "metadata": {},
   "source": [
    "**Common Patterns**\n",
    "\n",
    "In cases where performance is not close to theoretical limits there are often common patterns to be observed that provide and explanation:\n",
    "* **Thread divergence**: threads of the same warp take different code paths.\n",
    "* **(Un-)coalesced memory accesses**: consecutive threads access (non-)consecutive memory locations.\n",
    "* **Kernel launch overhead**: many short-running kernels lead to a low ratio of time spent in kernels over total execution time.\n",
    "* **Low occupancy**: lack of parallelism, and/ or overusing available resources lead to a low occupancy.\n",
    "\n",
    "**Additional Material**\n",
    "\n",
    "* Many computing centers (regional, national and beyond) around the world offer excellent trainings, often online and free of charge for members of academia.\n",
    "* [GTC](https://www.nvidia.com/gtc/) offers a wide spectrum of material around GPU computing. There is also an [on-demand archive](https://www.nvidia.com/on-demand/) with thousands of videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
